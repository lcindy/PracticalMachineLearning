Practical Machine Learning Project 
Project: Predict the manner of exercise
========================================================
##  Background::
The goal is to use data from accerlerometers on the belt, forearm, arm and dumbbell of 6 participants.  They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  This project is to use the data to predict the manner in which the 6 participants did the exercise.  The five behaviours are described as the following:  
A.Exact  
B.Throw elbow to the front  
C.Lift dumbbell half way  
D.Lower dumbbell half way  
E.Throw the hip to the front  

The data is extracted from the following website
http://groupware.les.inf.puc-rio.br/har

##  Data Analysis
### Explore the data:
The data set includes 52 variables as the following:  
1.Roll/pitch/yaw of belt/arm/forearm/dumbbell  
2.Total acceleration of  belt/arm/forearm/dumbbell  
3.Gyros/accel/magnet of belt/arm/forearm/dumbbell in x/y/z  
To clean the dataset, I first filter out the unnecessary columns and include new window = no.   
```{r,echo=FALSE, cache=TRUE}
library(caret)
setwd("D:/Coursera/DataScience/PracticalMachineLearning/Project")
training_data_set<-read.csv("pml-training.csv",head = TRUE)
testing_data_set<-read.csv("pml-testing.csv",head = TRUE)
#Clean Data
#Select variables
training_data_set<-training_data_set[training_data_set$new_window=='no',]
testing_data_set<-testing_data_set[testing_data_set$new_window=='no',]

variable_list<-names(training_data_set)
variable_list_logical<-rep(FALSE,length(variable_list))
filter<- c("roll_belt","pitch_belt","yaw_belt","total_accel_belt","gyros_belt_x","gyros_belt_y","gyros_belt_z","accel_belt_x","accel_belt_y","accel_belt_z","magnet_belt_x","magnet_belt_y","magnet_belt_z","roll_arm","pitch_arm","yaw_arm","total_accel_arm","gyros_arm_x","gyros_arm_y","gyros_arm_z","accel_arm_x","accel_arm_y"
           ,"accel_arm_z","magnet_arm_x","magnet_arm_y","magnet_arm_z","roll_dumbbell","pitch_dumbbell",
           "yaw_dumbbell","total_accel_dumbbell","gyros_dumbbell_x","gyros_dumbbell_y","gyros_dumbbell_z","accel_dumbbell_x",
           "accel_dumbbell_y","accel_dumbbell_z","magnet_dumbbell_x","magnet_dumbbell_y","magnet_dumbbell_z","roll_forearm",
           "pitch_forearm","yaw_forearm","total_accel_forearm","gyros_forearm_x","gyros_forearm_y","gyros_forearm_z",
           "accel_forearm_x","accel_forearm_y","accel_forearm_z","magnet_forearm_x","magnet_forearm_y","magnet_forearm_z","classe")

for (j in 1:length(variable_list)){
    for (i in 1:length(filter) ) {
        if(variable_list[j]==filter[i]) variable_list_logical[j]<-TRUE
    }
}

training_data<-training_data_set[,variable_list_logical]
testing_data<-testing_data_set[,variable_list_logical]
#head(training_data)
```

Then I partition the dataset into training set and cross validation set in the following percentage:    
Training: 70%  
Cross Validation: 30%  

```{r,echo=TRUE, cache=TRUE}
inTrain = createDataPartition(training_data$classe, p = 0.7)[[1]]
training = training_data[ inTrain,]
cv = training_data[-inTrain,]
#number of data points in training
dim(training)
#number of data points in cross validation 
dim(cv)
```
### Fitting the Model:
Random forest is chosen as the main algorithm for training.  The dataset has more than 16000 rows with 52 variables.  Random forest is well known for its accuracy, its ability to work with large data sets and to handle large set of variables.  In addition, it provides accurate information about the importantce of each variables.  Although it is a bit time consuming, I think it's the best model for this project.

```{r,echo=TRUE, cache=TRUE}
set.seed(33833)
#modfit <- train(classe ~ . , data = training, method="rf",prox=TRUE)
library(randomForest)
modfit<-randomForest(classe ~ . , data = training, importance = T)
modfit
plot(modfit)
```
  
I train the training set using randomForest package.  The resulting average error is 0.48%.  Based on the graph which plots the error rate for 500 trees.  The max error rate is around 15% while most trees have very small error rate.  Given that the average error is less than 0.48% and it is trained with a dataset with 13000 data points.  I expect the out of sample error rate to be potentiall small.  

In addition, by examining the confusion matrix.  Class D (lower dumbbell half way) has the largest error rate of 1%.  Class A (Exact) and Class E(throw the hip to the front) have the lowest error rate.  To further analyze the accuracy of the model, I also look into the variable importance.    

```{r,echo=TRUE, cache=TRUE}
variable_imp<-varImp(modfit)
#Class A
order.A<-order(variable_imp$A,decreasing=TRUE)
Top.A<-head(variable_imp[order.A,],8)
cbind(row.names(Top.A),Top.A$A)
```

The most important variables to identify Class A(Exact) are yaw_belt, magnet_dumbbell_z and roll_belt.  I have extracted the top 8 variables.  The sensor around the waist (in the belt area) seems to be the best way to identify the correct posture for barbell lift.  The movements in the forearm and dumbbell also contribute highly to identify Class A.
```{r,echo=TRUE, cache=TRUE}
#Class B
order.B<-order(variable_imp$B,decreasing=TRUE)
Top.B<-head(variable_imp[order.B,],5)
cbind(row.names(Top.B),Top.B$B)
```
The most important variables to identify Class B(Throw Elbow to the front) are pitch_belt, roll_belt and yaw_belt.  It is clearly seen that the movements on the waist (around the belt) play an important role to identify Class B.  

```{r,echo=TRUE, cache=TRUE}
#Class C
order.C<-order(variable_imp$C,decreasing=TRUE)
Top.C<-head(variable_imp[order.C,],5)
cbind(row.names(Top.C),Top.C$C)
```
Class C is "lift dumbbell half way".  magnet_dumbbell_z and magnet_dumbbell_y are the most important variables to identify Class C.  It is anticipated to see the movements in the dumbbell are most significant.
```{r,echo=TRUE, cache=TRUE}
#Class D
order.D<-order(variable_imp$D,decreasing=TRUE)
Top.D<-head(variable_imp[order.D,],5)
cbind(row.names(Top.D),Top.D$D)
```
Class D is "lower dumbbell half way".  It is expected to see dumbbell related variables have high importance.  However, it is very interesting to see the movements around the waist are significant.  
```{r,echo=TRUE, cache=TRUE}
#Class E
order.E<-order(variable_imp$E,decreasing=TRUE)
Top.E<-head(variable_imp[order.E,],5)
cbind(row.names(Top.E),Top.E$E)
```
Class E is "throwing the hip to the front".  It is anticipated that the movement around the waist area (the sensor around the belt) has outpaced the other variables and is most sigficant.  

After analyzing the importance of each variable for all five classes, along with the error rate of 0.48%.  I am confident that random forest model can be a good prediction for this exercise.  Thus, small out of sample error is anticipated.  

### Prediction/Out of Sample Test:
I have test the model with the cross validation set, which is 30% of the training data set. 
```{r,echo=TRUE, cache=TRUE}
pred<-predict(modfit,cv)
table(pred,cv$classe)
predRight<-pred==cv$classe
#Total of accurate prediction for out of sample data
accuracy<-sum(predRight)/length(predRight)
qplot(1:length(pred),pred, colour=predRight)
```
  
The out of sample error rate  is `r  (1-accuracy)*100`% which is very close to the sample error rate of 0.48% from the training set.  We can see from the plot that orange dots are the ones that I failed to predict.  

### Test Results:
Please see below for the prediction of the test class.
```{r,echo=TRUE, cache=TRUE}
test_classe<-predict(modfit,testing_data)
test_classe
```
  
### Conclusion:
The random forest model turns out to be a good model for predicting the correctness of barbell lifts.  The error rate is 0.48%.  By examing the importance of the variables, I am able to look into the contribution of each variable for each class.  When I implement the model with cross validation set, the out of sample error is `r  (1-accuracy)*100`% which is very satisfactory.
